
The R files contained in this directory will perform the tests described in 
the associated paper. 

multivariate_logistic_example.R:

For multivariate_logistic_example.R, numerous test runs will be performed, and 
many of the intermediate values are written to standard out. After each block
of tests, a final section will be printed with the summary results. 

It will look like this. 

[1] "Done[ 1000 ][ 5 ][ 2 ]"
[1] "T-Statistics[ 1000 ][ 5 ][ 2 ]:  24.7984265889912 -10.595144771372 8.16230090370061 -12.599649743057"
[1] "mle, l2, l1, ice"
[1]  2.651230e-03 -8.867270e-04  7.605901e-03 -6.667354e-05
              [,1]          [,2]          [,3]          [,4]
[1,]  3.429004e-06 -1.654971e-06 -2.721176e-06 -7.795261e-08
[2,] -1.654971e-06  2.101297e-06  4.150455e-06  7.230737e-08
[3,] -2.721176e-06  4.150455e-06  2.604938e-04  3.087446e-07
[4,] -7.795261e-08  7.230737e-08  3.087446e-07  8.400620e-09
            [,1]       [,2]        [,3]       [,4]
[1,]  1.00000000 -0.6165422 -0.09104875 -0.4592943
[2,] -0.61654225  1.0000000  0.17739991  0.5442310
[3,] -0.09104875  0.1773999  1.00000000  0.2087109
[4,] -0.45929427  0.5442310  0.20871089  1.0000000

The value for MLE (2.651230e-03 in the example above) is the difference between
the actual cross entropy for \theta_0 vs. the performance of the MLE estimate of
theta. The following (l2, l1 and ice) values are compared to MLE rather than theta_0. 

In this case, -6.66735e-05 (the ice value in this example) indicates that the ICE
estimator slightly outperformed MLE. The T-statistics are computed vs theta_0 
(for MLE) or vs MLE for other estimators. So the ICE estimator (in this example)
had a t-statistic of -12.599, indicating it was better than MLE to a very high
significance level. 


friedman_example.R:

For the friedman_example, the logic used is much simpler, so the output is simpler as well. 

Like the previous code, each iteration will print some intermediate results. The
summary at the end of each section looks like this: 

[1] "1024 ,  199 ,  0.00324483795969425 ,  2.14167790054511 ,  0.00309592569421112 ,  2.13843306258542 ,  -0.000148912265483123"
[1] "1024 ,  200 ,  0.0028442991979764 ,  4.13775815672524 ,  0.00247933089075078 ,  4.13491385752726 ,  -0.000364968307225612"
[1] "MLE KL:  0.00287795453397619"
[1] "L2 KL:  1.8230308033031  (worse by  1.82015284876912 ) [ 25.26868646405 ]"
[1] "ICE KL:  0.00283372420306125  (worse (or better if negative) by  -4.42303309149414e-05 ) [ -2.09463141477653 ]"
[1] "ICE vs. MLE Friedman-P:  0.0773047404432994"
[1] "ICE vs. L2 Friedman-P:  4.2241524062062e-39"
[1] "L2 vs. MLE Friedman-P:  1.08661064074595e-36"

You can see the last two iterations of the n=1024 series, and then the summary
results for this series. 

Here The KL values are printed directly, and all are KL-divergences between the 
estimated model and the true model represented by theta_0. T-statistics are in 
brackets (e.g. -2.094 for ICE in the above example), and differences between
KL divergences are noted in parentheses (e.g. ICE was better than MLE by -4.42e-5 
in the above example). Also, friedman's test was performed and the p-values are
provided here. In most cases, these values are statistically indistinguishable from
zero, but in the above example, it can be seen that there is a 7% chance that the ICE
results in this calculation are not actually better than MLE. This closely matches
with the -2.095 t-stat, as would be expected since this model has gaussian errors. 
